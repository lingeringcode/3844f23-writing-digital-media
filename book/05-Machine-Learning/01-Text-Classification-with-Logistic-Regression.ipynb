{"cells":[{"cell_type":"markdown","metadata":{"id":"GzTwa1dklgs5"},"source":["# Chapter 5.1 - Text Classification with Logistic Regression"]},{"cell_type":"markdown","metadata":{},"source":["Our first machine-learning challenge is to create a prediction model that categorizes news articles with the appropriate categories from a set of 31 categories: politics, entertainment, etc. The model uses logistic regression techniques in Python on a dataset with headlines, short descriptions, and URLs. \n","\n","*NOTE*: Be sure to watch and read the materials posted in the Canvas module before and while you work through this notebook.\n","\n","**Learning Objectives**\n","\n","1. Import and run EDA techniques to understand the potential limits and affordances of the dataset with our ML goal in mind.\n","2. Learn about the basic mechanics of logistic regression (LR).\n","3. Apply LR to this text classification goal of categorizing the news genre of articles based on potential \"features\" in the data, such as the article's headline, short description, and URL."]},{"cell_type":"markdown","metadata":{},"source":["**Sources**\n","\n","- Notebook modified from Ganesan's LR example exercise: [Text Classification with Logistic Regression](https://github.com/kavgan/nlp-in-practice/tree/master/text-classification)\n","- Dataset: HuffPost News Dataset in [../data/05-ML](../data/05-ML/news_category_dataset.json)"]},{"cell_type":"markdown","metadata":{"id":"qJnzMoFVlgs8"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# You may need to install some of the libraries below\n","# If so, uncomment any of the below commands\n","# !pip install matplotlib\n","# !pip install seaborn\n","# !pip install mplcyberpunk\n","# !pip install scikit-learn\n","# !pip install pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":149,"status":"ok","timestamp":1649708820376,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"q3fZFMM_lgs9"},"outputs":[],"source":["import pandas as pd\n","import regex as re\n","import numpy as np\n","import logging\n","\n","# Data visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import mplcyberpunk\n","%config InlineBackend.figure_formats = ['svg']\n","\n","# ML Modeling\n","from sklearn.metrics import precision_recall_fscore_support,confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.metrics import RocCurveDisplay,roc_curve\n","from sklearn.preprocessing import normalize\n","# Saving and importing trained models\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"8SmcYeEklgtB"},"source":["## 1. Refresher on pandas' dataframes"]},{"cell_type":"markdown","metadata":{},"source":["We can use the pandas library to read in, review, and revise the data set.\n","\n","Recall how pandas' \"panel data\" helps us more easily transform multiple types of structured data into what's called a *DataFrame*. Dataframe's are two-dimensional tabular data that are mutable (transformable) with labeled axes (rows and columns). See chapter 3 to refresh your memory, if needed. You can also reference [pandas in general](https://pandas.pydata.org/docs/getting_started/index.html#getting-started), or its [Dataframe datatype](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame).\n","\n","<figure>\n","<img src=\"../images/01_table_dataframe.svg\"\n","     style=\"max-width: 300px; background: white;\"\n","/>\n","<figcaption>Panel data \"Dataframe\" that shows how there are rows for observations and columns for properties of those observations, as well as an index column for easy reference per row.\n","</figure>"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Import and Review the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":991,"status":"ok","timestamp":1649709032342,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"wK1wBFMFlgtC"},"outputs":[],"source":[" \n","# Imports the noted JSON file as a pandas DataFrame based on the path below\n","df = pd.read_json(\"./../data/05-ML/news_category_dataset.json\", lines=True)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1 General shape and content of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.shape"]},{"cell_type":"markdown","metadata":{},"source":["`.shape` returns a tuple with the panel data 2-dimensional info: 1. the number of rows, and 2. the number of columns."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1649709032518,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"7zw_3EQPlgtC","outputId":"63a502ab-5e06-4d8d-c230-3578444ff853"},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{},"source":["Looks like there are no `null` values in the dataset.\n","\n","Let's dig deeper into the columns and values."]},{"cell_type":"markdown","metadata":{"id":"B6USsXeFlgtF"},"source":["### 2.2 Review particular columns"]},{"cell_type":"markdown","metadata":{},"source":["Be sure to review the data per column. The goals of this modeling may be set for you, but you will be asked to perform similar EDA work on your final project, so you can better understand the modeling possibilities and boundaries with your data. So, the below includes a series of exercises for you to understand this dataset before you conduct the actual modeling work."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1649709032655,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"3dzu8AdylgtF","outputId":"60bad5c1-1fb4-4e4f-f679-e4d4c1493f07"},"outputs":[],"source":["# List the columns for reference\n","df.columns"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.1 Describe and examine the `headline` column"]},{"cell_type":"markdown","metadata":{},"source":["We can use `.describe()` on a Series, so we can consider any potential quirks. \n","\n","Let's check out the `headline` column, since that's an important column for training our model."]},{"cell_type":"markdown","metadata":{},"source":["##### What are the summary stats for the `headline`?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":389},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1649709032519,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"16vrEVmAlgtD","outputId":"f9d9c0dc-5a57-4521-b2d1-dacb03c32ae7"},"outputs":[],"source":["df.headline.describe()"]},{"cell_type":"markdown","metadata":{},"source":["Some initial observations: \n","\n","- Noticing how there is a decently sized difference between the `count` and `unique` values\n","- `top`: How \"Sunday Roundup\" headline appears 90 times (`freq`) in the `headline` column (`Name: headline, dtype: object`).\n","\n","Let's review rows with the \"Sunday Roundup\" value in the headline column by using our new skills with pandas. Below I ...\n","\n","1. `df.loc[]`: query the dataframe with the location method\n","2. `[df.headline]`: Specifiy what slice of the data I want to isolate.\n","3. `.str.contains('Sunday Roundup')`: Since `headline` values are Strings, and I want to isolate any rows with the \"Sunday Roundup\" headline, search the isolated Series, `headline`,  with the `.str.contains()` method. It takes a string as its main parameter."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.loc[df.headline.str.contains('Sunday Roundup')].sample(3)"]},{"cell_type":"markdown","metadata":{},"source":["##### How long are the headlines? (What's the distribution?)\n","\n","Also curious about the distribution of the length of those headlines, so let's add a new column to the `df` by applying the `len` (length) method to the `headline` column. Below, I do so with the `apply()` method and assign the values per row to a new Series (column) that I call `headline_length`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['headline_length'] = df.headline.apply(len)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''\n","  .hist() -- Creates a histogram chart with a dataframe's Series \n","  \n","  Histograms place a metric in bins -- dates in this case -- to understand the distribution of the data. In this case, the distribution of the data over time\n","'''\n","df.headline_length.hist(\n","  figsize=(12,6),\n","  color='#86bf91',\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Looks like there's some variance with some headlines that are over 100 characters long and as much as ~300.\n","\n","Let's use `.describe()` on this new column as a table of values to review."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.headline_length.describe()"]},{"cell_type":"markdown","metadata":{},"source":["##### 2.2.1 Exercise -- Observations about the headlines column\n","\n","- ***ENTER YOUR FIRST OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER YOUR SECOND OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER MORE OBSERVATIONS***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***"]},{"cell_type":"markdown","metadata":{"id":"eSFKju2BlgtG"},"source":["#### 2.2.2 What are the range and distribution of dates?"]},{"cell_type":"markdown","metadata":{},"source":["Since we have a `date` column, and the datatype is in a standardized format, we can quickly plot the date range in a histogram figure.\n","\n","Let's jot down some observations in our notebook.\n","\n","**NOTE**: Be sure to respond to the questions and add at least one more potential question, observation, and potential explanation for it"]},{"cell_type":"markdown","metadata":{},"source":["##### 2.2.2.1 *EXERCISE* -- Notes on the distribution of the data based on the `date` column"]},{"cell_type":"markdown","metadata":{},"source":["- Articles' publishing dates range between July 2014 and July 2018\n","  - Question: How might this impact the model's output?\n","  - ***YOUR RESPONSE HERE***\n","- Fewer 2018 articles compared to the rest of the dataset.\n","  - Question: How might this impact the model's output?\n","  - ***YOUR RESPONSE HERE***"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":369,"status":"ok","timestamp":1649709033022,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"7MlAJyRblgtG","outputId":"a4b4d7b1-d885-423e-8135-fbd69e10f7c5"},"outputs":[],"source":["'''\n","  .hist() -- Creates a histogram chart with a dataframe's Series \n","  \n","  Histograms place a metric in bins -- dates in this case -- to understand the distribution of the data. In this case, the distribution of the data over time\n","'''\n","\n","df.date.hist(\n","  figsize=(12,6),\n","  color='#86bf91',\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.3 Describe and examine the `category` column of news genres in the data"]},{"cell_type":"markdown","metadata":{},"source":["Use *dot notation* and a *column name* with `sample()`` to sample values in that column (Series).\n","\n","**Remember**: The dataframe stays the same because we are not altering `df`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1649709032655,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"pnlQ_4KWlgtF","outputId":"ac8f9318-bd51-4f5e-ed1e-4009c90f6038"},"outputs":[],"source":["df.category.sample(5)"]},{"cell_type":"markdown","metadata":{},"source":["Now, let's read and understand the news genre categories (politics, entertainment, etc.) in the dataset, as well as their distribution."]},{"cell_type":"markdown","metadata":{},"source":["##### 2.2.3.1 Examine the `category` counts and values"]},{"cell_type":"markdown","metadata":{},"source":["We can use the `len()` and `set()` functions in Python to isolate the unique number of values in a column. \n","\n","`len()` by itself would provide a length of *all values combined*, even if the value is repeated. By using `set()` first on the column, we can tell Python to reduce the column to unique set of values. Then, it will count that unique set with `len()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649709033022,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"WmxpY6RzlgtG","outputId":"bc1ff705-4ee4-4b03-a2a9-c35ff4e973ca"},"outputs":[],"source":["len(\n","  set(df['category'].values)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649709033023,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"su_oZCw3lgtH","outputId":"9f85a984-6255-4a13-cc3e-a7e0f72c4aa1"},"outputs":[],"source":["set(\n","  df['category'].values\n",")"]},{"cell_type":"markdown","metadata":{"id":"yoLCJSPvlgtH"},"source":["##### 2.2.3.2 `category` by count"]},{"cell_type":"markdown","metadata":{},"source":["Let's review the distribution of categories, since this could impact the model.\n","\n","In the code below, we tell Python to:\n","\n","1. Select the `category` column,\n","2. Count the values of each value in the column with `value_counts()`,\n","3. Plot the results with `plot` and provide the value `bar` for its parameter `kind`"]},{"cell_type":"markdown","metadata":{},"source":["##### 2.2.3.3 *EXERCISE* -- Notes on the distribution of the data based on the `category` column"]},{"cell_type":"markdown","metadata":{},"source":["- ***ENTER YOUR FIRST OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER YOUR SECOND OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER MORE OBSERVATIONS***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1649709033450,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"YIC6dO5tlgtH","outputId":"094d2b3e-d436-4025-e984-db260c7e85b1"},"outputs":[],"source":["df['category'].value_counts().plot(kind='bar')"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.4 Describe and examine the `short_description` column"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.short_description.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['sd_length'] = df.short_description.apply(len)\n","df[['short_description','sd_length']].sample(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.sd_length.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.sd_length.hist(\n","  figsize=(12,6),\n","  color='#86bf91',\n",")"]},{"cell_type":"markdown","metadata":{},"source":["##### 2.2.4.1 *EXERCISE* -- Notes on the `sd_length` column"]},{"cell_type":"markdown","metadata":{},"source":["- ***ENTER YOUR FIRST OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER YOUR SECOND OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER MORE OBSERVATIONS***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***"]},{"cell_type":"markdown","metadata":{"id":"fOHLfyNmlgtH"},"source":["## 3. Process the Data for Classification"]},{"cell_type":"markdown","metadata":{},"source":["Some of the fields will be useful to use for the classification task.\n","\n","1. `headline`: Since we're writing a model to classify headlines, this column is integral to our goal.\n","2. `short_description`: This column includes a short descrtiption of the story, which will provide potentially helpful contextual information to make a better model for our genre classifier. We'll see!\n","3. `tokenized_url`: This column includes information too! Different parts of URLs often capture how people have organized and classified information. So, the URL may also provide potentially helpful contextual information to make a better model. Again, we'll see!\n","\n","The code below creates 3 new columns for that task.\n","\n","There's a lot to unpack below, but it basically\n","\n","1. normalizes the URL for each article via the `tokenize_url()` function, which strips the URL down to the portion where the URL conveys article-specific information. Then, \n","2. it creates 3 new columns that are combined with the available data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1698,"status":"ok","timestamp":1649709035143,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"-ahlrsvSlgtH","outputId":"cd838282-f8ef-491c-8934-485512ea3499"},"outputs":[],"source":["def tokenize_url(url:str):\n","    url=url.replace(\"https://www.huffingtonpost.com/entry/\",\"\")\n","    url=re.sub(\"(\\W|_)+\",\" \",url)\n","    return url\n","\n","df['tokenized_url']=df['link'].apply(lambda x:tokenize_url(x))\n","\n","#just the description\n","df['text_desc'] = df['short_description']\n","\n","#description + headline\n","df['text_desc_headline'] = df['short_description'] + ' '+ df['headline']\n","\n","#description + headline + tokenized url\n","df['text_desc_headline_url'] = df['short_description'] + ' '+ df['headline']+\" \" + df['tokenized_url']\n","\n","df.info()\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's sample the new row with the URL info to see what it includes.\n","\n","##### EXERCISE -- Notes on the on the `text_desc_headline_url` column\n","\n","- ***ENTER YOUR OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER MORE OBSERVATIONS***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['text_desc_headline_url'].sample(5).values.tolist()"]},{"cell_type":"markdown","metadata":{"id":"L6kJ6vitlgtI"},"source":["## 4. Train a Logistic Regression Model"]},{"cell_type":"markdown","metadata":{},"source":["### 4.1 What's a Logistic Regression Model?"]},{"cell_type":"markdown","metadata":{},"source":["Logistic regression returns a probability that something may or may not happen or \"be\". We can use this probability as a single value to assess the likelihood of the event/being, such as \"This X headline is of Y news genre\".\n","\n","So, let's say our LR model returns a value of 0.990 for a particular headline's news genre as being \"POLITICS\". This probability score is very likely to accurately predict that this headline is indeed an article about POLITICS. Conversely, another headline with a prediction score of 0.005 on that same logistic regression model is very likely not about POLITICS. Yet, what about a headline with a prediction score of 0.6? \n","\n","In this lesson, our LR model uses those probability estimates as a binary category. To do so, we must decide what's called a \"classification threshold\" or \"decision threshold\". Any value above that threshold indicates a headline is about POLITICS, and any value below the threshold indicates that the headline is not POLITICS, but some other news genre.\n","\n","The default decision threshold in the scikit-learn code library that we will use is 0.5. But, this library also enables us to \"tune\" the LR model based on our problem-dependency / context, as well as take the best/top probability score to predict the news genre of the input headline."]},{"cell_type":"markdown","metadata":{},"source":["### 4.2 How to Train a Drago... Erm ... a LR Model!"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.2.1 *From Words to Numbers!* - Transform a corpus of words into matrix of integers with `CountVectorizer()`"]},{"cell_type":"markdown","metadata":{},"source":["**Before you review the many functions defined in section 4.2 below**, functions that will automate the modeling process, I want you to understand a very important first step: how we get from word label data to numbers for the model. After all, this is \"Writing & Digital Media\" **:-)**\n","\n","So, first things first, we're working with text, so strings and characters. But, a logistic regression model requires numbers. So, we need to extract the features of our corpus and transform them into a data type that can be quantified.\n","\n","`CountVectorizer()` to the rescue!"]},{"cell_type":"markdown","metadata":{},"source":["To better understand how scikit-learn's built-in functions transform the textual data for us, I provide us with a tiny tiny case of a list of sentences, which transforms our textural data into a **sparse matrix** of values that indicate the frequency of times the word (feature/token) is present in a unit of content (document). \n","\n","In this tiny case below, our documents are pretty short strings in a list. Those could be much much bigger, and really it should be for this type of modeling. But, for example purposes, let's see how the matrix logs how many instances a word shows up in each document. \n","\n","Each row is the document, (i.e., sentence, in this case), while each column is the feature (word/token). So the values at the intersection of the row and column in the matrix report the raw frequency count (integer)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Tiny weeny bitty small corpus :-)\n","list_test_text_1 = ['Hello my name is chris', 'chris this is my python notebook you know python']\n","# list_test_text_2 = ['Hello my name is chris', 'chris this is my python notebook', 'chris trying to create a big dataset', 'chris is trying different words', 'features of count vectorizer']\n","\n","# Instantiate an sklearn CountVectorizer (CV) object\n","test_cv_object = CountVectorizer()\n","\n","# Use the CV's fit_transform() function to count the frequency of each feature (word/token as columns) in each document (rows)\n","test_count_matrix = test_cv_object.fit_transform(list_test_text_1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Note the specialized data type for sklearn\n","type(test_count_matrix)"]},{"cell_type":"markdown","metadata":{},"source":["If I try to print out the sklearn cv matrix, I'll get some basic data structural info."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_count_matrix"]},{"cell_type":"markdown","metadata":{},"source":["The matrix has 2 rows (documents/sentences) and 10 columns (words/tokens) with values that are numpy integers.\n","\n","But, what's the now transformed/quantified data look like? Let's convert this returned sparse matrix as a more recognizable data type: a pandas dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert the matrix into a more basic Python array\n","test_count_array = test_count_matrix.toarray()\n","\n","# Use that array as the data in the DF, then nicely use CV's `get_feature_names_out()` method to provide the column names\n","test_cv_df = pd.DataFrame(\n","  data=test_count_array,\n","  columns=test_cv_object.get_feature_names_out()\n",")\n","test_cv_df"]},{"cell_type":"markdown","metadata":{},"source":["**What do we notice?**\n","\n","- 2 rows\n","- 10 columns\n","- values are the frequency of times a word shows up in a document, such as \n","  - `chris`, `is`, and `my` are in both docs\n","  - `python` shows up twice in the second document\n","  - Zeros indicate the word doesn't show up in a document"]},{"cell_type":"markdown","metadata":{},"source":["##### Summarize `CountVectorizer()`"]},{"cell_type":"markdown","metadata":{},"source":["In the modeling functions below, the extract_features() function uses CountVectorizer() to, well, extract these features from the much larger corpus. It nicely automates the process for us—writing and calculating at amazing speed and scale. **Can you imagine transforming the data manually?!** I don't think we want to subject ourselves to even such a thought."]},{"cell_type":"markdown","metadata":{},"source":["### 4.3 Modeling functions\n","\n","The functions below help us create a systematic and reproducable workflow to train the data.\n","\n","Be sure to check out my videos that walk through an overview of what they do."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1649709035144,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"TkesOTjPlgtI"},"outputs":[],"source":["def _reciprocal_rank(true_genre_labels: list, machine_predicted_genre_labels: list):\n","    '''\n","    ## Purpose\n","    Compute the reciprocal rank at cutoff k\n","\n","    ## Parameters\n","        - `true_genre_labels` (List): List of actual news genre labels\n","        - `machine_predicted_genre_labels` (List): List of news genre labels predicted by the LR algorithm\n","    \n","    ## Return Values\n","        - `recip_rank` (Float): Reciprocal rank\n","    '''\n","    \n","    # add index to list only if machine predicted label exists in true labels\n","    tp_pos_list = [(idx + 1) for idx, r in enumerate(machine_predicted_genre_labels) if r in true_genre_labels]\n","\n","    recip_rank = 0\n","    if len(tp_pos_list) > 0:\n","        # for reciprocal rank we must find the position of the first **correctly labeled** item\n","        first_pos_list = tp_pos_list[0]\n","        \n","        # recip_rank = 1/rank\n","        recip_rank = 1 / float(first_pos_list)\n","\n","    return recip_rank\n","\n","def compute_mrr_at_k(eval_news_category_items:list):\n","    '''\n","    ## Purpose\n","    `compute_mrr_at_k()`: Computes the MRR (average RR) at cutoff k. In sum, it takes the mean average of all of the reciprocal rank scores among the actual vs. predicted labels. Review this [\"Mean reciprocal rank\" wikipedia article](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) for a simple explainer.\n","    ## Parameters\n","    - `eval_news_category_items` (List): List that contains 2 values\n","        1. String - Actual news genre category\n","        2. List of strings - Predicted news genre category in order by estimated probability to be returned by the model.\n","            - The example below shows how \n","                - `'HEALTHY LIVING'` was the actual label, but it was third in 'reciprocal rank' with a value of 1/3\n","                - `'WORLDPOST'` was the actual label, and it was first in 'reciprocal rank' with a value of 1\n","                \n","                [\n","                    [\n","                        ['HEALTHY LIVING'], ['POLITICS', 'ENTERTAINMENT', 'HEALTHY LIVING']\n","                    ], \n","                    [\n","                        ['WORLDPOST'], ['WORLDPOST', 'MEDIA', 'POLITICS']\n","                    ], \n","                    ...\n","                ]\n","\n","    ## Return Values\n","        - `mean_reciprocal_rank_score` (Float): Mean average reciprocal rank score among the predicted news category in the model\n","    '''\n","    rr_total = 0\n","    \n","    for item in eval_news_category_items:\n","        actual_label = item[0]\n","        pred_label_list = item[1]\n","\n","        # Find the reciprocal rank (RR) for this row\n","        rr_at_k = _reciprocal_rank(actual_label, pred_label_list)\n","\n","        # Add the row's RR to the accruing scores for the entire corpus\n","        rr_total = rr_total + rr_at_k\n","\n","        # Update the Mean Reciprocal Rank (MRR) score with new row value\n","        mean_reciprocal_rank_score = rr_total / 1/float(len(eval_news_category_items))\n","\n","    return mean_reciprocal_rank_score\n","\n","def collect_preds(Y_test, Y_preds):\n","    '''\n","    ## Purpose\n","    Collect all predictions (predicted news genre labels) and ground truth (i.e., actual news genre label)\n","    '''\n","    pred_gold_list = [ [ [Y_test[index]], pred ] for index, pred in enumerate(Y_preds) ]\n","    return pred_gold_list\n","             \n","def compute_accuracy(eval_news_category_items:list):\n","    '''\n","    ## Purpose\n","    `compute_accuracy()`: Compute the overall accuracy score of the model across the training corpus\n","\n","    ## Parameters\n","        - `eval_news_category_items` (List): List that contains 2 values\n","            1. String - Actual news genre category\n","            2. List of strings - Predicted news genre category\n","\n","            Example: [\n","                [\n","                    ['HEALTHY LIVING'], ['POLITICS', 'ENTERTAINMENT', 'HEALTHY LIVING']\n","                ], \n","                [\n","                    ['WORLDPOST'], ['WORLDPOST', 'MEDIA', 'POLITICS']\n","                ], \n","                ...\n","            ]\n","    ## Return Values\n","        - `news_cat_prediction_accuracy` (Float): Percentage of accurately predicted news category in the model\n","    '''\n","    correct_news_cat = 0\n","    \n","    for news_genre_cat in eval_news_category_items:\n","        true_pred = news_genre_cat[0]\n","        machine_pred = set(news_genre_cat[1])\n","        \n","        for news_cat in true_pred:\n","            if news_cat in machine_pred:\n","                correct_news_cat += 1\n","                break\n","    \n","    news_cat_prediction_accuracy = correct_news_cat / float(len(eval_news_category_items))\n","    return news_cat_prediction_accuracy\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":183,"status":"ok","timestamp":1649709035322,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"My5nLBMXlgtI"},"outputs":[],"source":["logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","def extract_features(df, field, training_data, testing_data, type='binary'):\n","    '''\n","    ## Purpose\n","    `extract_features()`: Extract features using different method types: binary, counts, or TF-IDF\n","\n","    ### IF BINARY FEATURE REPRESENTATION\n","    Creates a new `CountVectorizer()` method object, which converts a collection of text documents to a matrix of token counts. Logistic regression involves vectorizing weighted averages of these tokens.\n","    '''\n","    \n","    logging.info(\"Extracting features and creating vocabulary...\")\n","\n","    '''\n","        BINARY and COUNTS PROCESSES WILL DO THE FOLLOWING:\n","\n","        sklearn's CountVectorizer() will convert text to numerical data.\n","    '''\n","    \n","    if 'binary' in type:\n","        \n","        # BINARY FEATURE REPRESENTATION\n","        # Creates a new CountVectorizer() method object, which can help us use built-in functions that convert a collection of text documents to a matrix of token counts. **REMEMBER** that logistic regression involves vectorizing weighted averages of these tokens.\n","        # NOTE: `max_df` == \"Maximum Document Frequency. It enables us to programmatically ignore frequently occuring words, e.g., articles like 'a' or 'the'. `max_df` reviews how many documents contain the word, and if it exceeds the max_df threshold then it is eliminated from the sparse matrix. Below we set the threshold to 95%.\n","        cv = CountVectorizer(binary=True, max_df=0.95)\n","        # CountVectorizer()'s fit_transform() uses the training_data to learn the vocabulary dictionary and return document-term matrix.\n","        cv.fit_transform(training_data[field].values)\n","        # CountVectorizer()'s transform() \n","        train_feature_set = cv.transform(training_data[field].values)\n","        test_feature_set = cv.transform(testing_data[field].values)\n","        \n","        return train_feature_set,test_feature_set,cv\n","  \n","    elif 'counts' in type:\n","        \n","        # COUNT BASED FEATURE REPRESENTATION\n","        cv = CountVectorizer(binary=False, max_df=0.95)\n","        cv.fit_transform(training_data[field].values)\n","        \n","        train_feature_set = cv.transform(training_data[field].values)\n","        test_feature_set = cv.transform(testing_data[field].values)\n","        \n","        return train_feature_set,test_feature_set,cv\n","    \n","    elif 'tfidf':    \n","        \n","        # TF-IDF BASED FEATURE REPRESENTATION\n","        tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95)\n","        tfidf_vectorizer.fit_transform(training_data[field].values)\n","        \n","        train_feature_set=tfidf_vectorizer.transform(training_data[field].values)\n","        test_feature_set=tfidf_vectorizer.transform(testing_data[field].values)\n","        \n","        return train_feature_set,test_feature_set,tfidf_vectorizer\n","\n","def get_top_k_predictions(model, X_test, k):\n","    '''\n","    ## Purpose\n","    `get_top_k_predictions()`: Uses the input trained LogisticRegression model to return the news genre class/category with the top estimated probability score.\n","    ## Parameters\n","    - `model` (LogisticRegression()): Trained model scikit-learn object\n","    - `X_test` (pandas DataFrame): Sampled test data set returned by `training_test_split()` in the `training_model()` function\n","    - `k` (Integer): Number of top categories (news genres) to return based on the estimated probability to predict the news genre\n","    ## Return Value(s)\n","    - `preds` (List of list): A list within a list of the top k retruned news categories. For example:\n","        - `preds` is `[['SCIENCE', 'HEALTHY LIVING', 'GREEN']]` for an article with the headline of `\"Exercise in space keeps astronauts from fainting when they return to Earth, study says\"` and `k=3`\n","    '''\n","    \n","    # get probabilities instead of predicted labels, since we want to collect top 3\n","    probs = model.predict_proba(X_test)\n","\n","    # GET TOP K PREDICTIONS BY PROB - note these are just index\n","    best_n = np.argsort(probs, axis=1)[:,-k:]\n","    \n","    # GET CATEGORY OF PREDICTIONS\n","    preds = [[model.classes_[predicted_cat] for predicted_cat in prediction] for prediction in best_n]\n","    \n","    preds = [ item[::-1] for item in preds]\n","    \n","    return preds\n","   \n","def train_model(df, field=\"text_desc\", feature_rep=\"binary\", top_k=3):\n","    '''\n","    ## Purpose\n","    train_model() is the main controller function that conducts the following modeling procedure: \n","        \n","    1. Create X data (List) by splitting the data to create two sampled sets: 1) for training, and 2) for testing.\n","    2. Create Y data (List) by assigning the actual (ground truth) labels\n","    3. Extract the features for the model to use, based on the chosen feature representation: binary vs. TF-IDF\n","    4. Fit, i.e., train, the logistic regression classifier model with scikit-learn's `LogisticRegression()` object\n","    5. Retrieve the evaluation items, e.g., the actual labels (ground truths) and predicted labels (list of top `k` number of estimated probable predicted categories)\n","    6. Use the evaluation iitems to compute the overall accuracy score and mean reciprocal rank score of the model\n","\n","    ## Parameters\n","    - `df` (pandas DataFrame): the complete data set / corpus\n","    - `field` (String): the column name of the feature used to train the model\n","    - `feature_rep` (String): Type of LR analysis set as either \"binary\" or \"count\" or \"tfidf\"\n","    '''\n","    \n","    logging.info(\"Starting model training...\")\n","    \n","    # 1. GET A TRAIN TEST SPLIT (set seed for consistent results)\n","    # train_test_split() from sklearn \"splits arrays or matrices into random train and test subsets.\"\n","    # returns 2 new dataframes: one for training, another for testing the trained model\n","    y = df['category']\n","    x_training_data,x_testing_data = train_test_split(\n","        df,\n","        random_state=2000 #Controls the shuffling applied to the data before applying the split\n","    )\n","\n","    # 2. GET LABELS FROM SPLIT DATA\n","    # Get the category values from each split data returned by #1\n","    Y_train = x_training_data['category'].values\n","    Y_test = x_testing_data['category'].values\n","     \n","    # 3. GET FEATURES\n","    X_train,X_test,feature_transformer = extract_features(\n","        df,\n","        field,\n","        x_training_data,\n","        x_testing_data,\n","        type=feature_rep\n","    )\n","\n","    # INITIALIZE THE LOGISTIC REGRESSION CLASSIFIER OBJECT\n","    logging.info(\"Training a Logistic Regression Model. This may take a few minutes. ...\")\n","    scikit_log_reg = LogisticRegression(\n","        verbose=0, #if you want the LR method to print out all the details, change this 0 to 1\n","        solver='liblinear',\n","        random_state=0,\n","        C=5,\n","        penalty='l2',\n","        max_iter=1000\n","    )\n","    # Create the model by providing the LR object the \n","    model = scikit_log_reg.fit(X_train, Y_train)\n","\n","    # GET TOP K PREDICTIONS\n","    preds = get_top_k_predictions(model, X_test, top_k)\n","    \n","    # GET PREDICTED VALUES AND GROUND TRUTH INTO A LIST OF LISTS - for ease of evaluation\n","    eval_items = collect_preds(Y_test, preds)\n","    \n","    # GET EVALUATION NUMBERS ON TEST SET -- HOW DID WE DO?\n","    logging.info(\"Starting evaluation...\")\n","    simple_mean_avg_correct_prediction_accuracy = compute_accuracy(eval_items)\n","    mean_recip_rank_at_k = compute_mrr_at_k(eval_items)\n","    \n","    logging.info(\"Done training and evaluation.\")\n","\n","    # Return the herein computed model and other values for potential use and exploration\n","    return model,feature_transformer,simple_mean_avg_correct_prediction_accuracy,mean_recip_rank_at_k,X_train,X_test,Y_test,Y_train,preds,eval_items\n"]},{"cell_type":"markdown","metadata":{"id":"9WPZ0JvLlgtI"},"source":["### 4.4 LR Model 1 - Binary features with `text_desc` only"]},{"cell_type":"markdown","metadata":{},"source":["Train the model based on the short description data.\n","\n","**FYI**. Your functions can return multiple variables, if you'd like. \n","\n","If you review the `train_model()` function, it returns multiple variables in a specific order. Those mirror the variables and the comma-separated variables and their desired types below."]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.1 Enact the Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":320275,"status":"ok","timestamp":1649709355588,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"_wWtByIplgtI","outputId":"a2f40cf2-8bca-4187-a9f4-84302d1631d1"},"outputs":[],"source":["# Parameters to configure for our `train_model()` function\n","training_field = 'text_desc' #use the short description only to train a model\n","feature_rep = 'count' # Specify if this model should use a binary approach to the features (0 or 1) or the actual counts created by CountVectorizer()\n","top_k = 3 #tell the model function to return the top 3 'best fits' among the distributed probabilities\n","\n","# Train that supervised ML logistic regression model!\n","model_td_only,transformer_td_only,accuracy_td_only,mrr_at_k_td_only,X_train,X_test,Y_test,Y_train,preds,eval_items = train_model(\n","  df, # full corpus\n","  field=training_field,\n","  feature_rep=feature_rep,\n","  top_k=top_k\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### Model Assessment Roadmap\n","\n","In the following subsections, you will examine the LR model that you just trained by:\n","\n","1. Print out the overall accuracy scores for the model\n","2. Compare the predictive power of the model across the news genre categories, i.e., *classes*\n","3. Visualize the predictive power per class/category with a \"confusion matrix\" heatmap\n","4. Dig deeper into the categories by comparing some categories' performance against the EDA work on the data set used to train the model"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.2 Test the accuracy/performance of the model"]},{"cell_type":"markdown","metadata":{},"source":["##### 4.4.2.1 See the accuracy and Mean Reciprocal Rank Scores"]},{"cell_type":"markdown","metadata":{},"source":["We already computed the overall accuracy and MRR scores, when we trained the model, so let's output them.\n","\n","If you need to refresh yourself on these scores, check out the functions' documentation in the 4.3 section."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Simple Mean Average Model Accuracy = {accuracy_td_only}\\nMean Reciprocal Rank = {mrr_at_k_td_only}\")"]},{"cell_type":"markdown","metadata":{},"source":["##### 4.4.2.2 Compare Actual Labels (ground truths) with Predicted Labels (with the k threshold)"]},{"cell_type":"markdown","metadata":{},"source":["Before we test our model with headline inputs, we can test its performance with scikit-learn's `predict_proba()` associated function with the output model.\n","\n","This function returns the estmated probability of each categorical label (news genre) in the test sample. With this returned data set, we can visualize the results to see what categories might be mislabeled more often than others and, of course, which categories are performing well.\n","\n","Here is a description of parameter and return data:\n","- `X_test`: sample of the original data to test the trained model\n","- `Y_probability_td_only`: List of the predicted news genre category outputs (String)\n","- `Y_probability_a_td_only`: List of lists of the predicted news genre category based on the estimated probability score\n","  - First list == 1 row from X_test data set\n","    - Lists within that list == Each list contains estimated probability scores (Float) per (31) news genre categories"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict the classes on the test data\n","Y_predictions_td_only = model_td_only.predict(X_test)\n","# Predict the classes on the test data, and return the probabilities for each class\n","Y_probability_a_td_only = model_td_only.predict_proba(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.3 Visualize Actual vs. Prediction with a Confusion Matrix"]},{"cell_type":"markdown","metadata":{},"source":["A confusion matrix helps you organize a direct comparison across all of your possible categories. Essentially, it asks: \n","\n","1. How many times did X (actual) equal the predicted (Y)?\n","2. How many times did X (actual) not equal the predicted (Y)?\n","\n","Another way to phrase the questions is to think of it in action. So, with this data set, there are 31 possible categories (news genres), which means we have 31 categories to test against each other—**that's 961 categories** (31 * 31 = 961)!"]},{"cell_type":"markdown","metadata":{},"source":["A confusion matrix maps what are called the True Positive, False Positive, False Negative, and True Negative prediction outcomes. In the table below, I provide a high-level description of what each of these values represent in the scheme of our LR model on predicting news genres."]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"max-width:700px;\">\n","  <table border=\"1\">\n","  <tbody><tr>\n","    <td style=\"background:lightgreen\" width=\"50%\">\n","      <b>True Positive (TP):</b>\n","      <ul>\n","        <li>Reality: Input SCIENCE headline</li>\n","        <li>LR predicted: \"SCIENCE\"</li>\n","        <li>Outcome: LR's prediction is correct/accurate</li>\n","      </ul>\n","    </td>\n","    <td style=\"background:pink\">\n","      <b>False Positive (FP):</b>\n","      <ul>\n","        <li>Reality: Input is <strong>NOT</strong> a SCIENCE headline</li>\n","        <li>LR predicted: \"SCIENCE\"</li>\n","        <li>Outcome: LR's prediction is incorrect/inaccurate</li>\n","    </ul></td>\n","  </tr>\n","  <tr>\n","    <td style=\"background:pink\">\n","      <b>False Negative (FN):</b>\n","      <ul>\n","        <li>Reality: Input SCIENCE headline</li>\n","        <li>LR predicted: <strong>NOT</strong> \"SCIENCE\"</li>\n","        <li>Outcome: LR's prediction is incorrect/inaccurate</li>\n","      </ul>\n","    </td>\n","    <td style=\"background:lightgreen\">\n","      <b>True Negative (TN):</b>\n","      <ul>\n","        <li>Reality: Input is <strong>NOT</strong> a SCIENCE headline</li>\n","        <li>LR predicted: <strong>NOT</strong> \"SCIENCE\"</li>\n","        <li>Outcome: LR's prediction is correct/accurate</li>\n","      </ul>\n","    </td>\n","  </tr>\n","</tbody></table></div>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["Ok, so let's visualize the TPs, FPs, FNs, and TNs as a heatmap with annotated values for quick reference."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cm_td_only = confusion_matrix(\n","  Y_test, #Sorted List of ground truth (correct/actual) target values\n","  Y_predictions_td_only #Sorted List of estimated targets as returned by a classifier\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Let's be sure to **normalize the confusion matrix values**.\n","\n","The current values in the confusion matrix are simply counts (Integers). But, if we want to visualize the prediction efficacy as a heatmap, we first need to \"normalize\" the values by converting them to a score relative to all of the other comparisons in the same column. For example, `ARTS` has results across 31 categories"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Normalize matrix by columns\n","cm_td_only_normed_by_column = normalize(cm_td_only, axis=0, norm='l1')\n","# Compare the original to the normalized\n","print(\n","  'Simple Counts of the ARTS (first) column:\\n', cm_td_only[0],'\\n\\n',\n","  'Normalized Counts of the ARTS (first) column:\\n',cm_td_only_normed_by_column[0],\n",")"]},{"cell_type":"markdown","metadata":{},"source":["##### 4.4.3.2 Heatmap of the `text_desc` only normalized confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(31,14))\n","\n","sns.heatmap(\n","  cm_td_only_normed_by_column, #normalized confusion matrix\n","  annot=True, # add normalized counts of co-occurrences between actual vs. predicted\n","  fmt=\".3f\", # round to thousandths decimal place\n","  linewidths=1, # style choice for row/column lines\n","  square=True, # make the \n","  cmap='Purples',\n","  xticklabels=model_td_only.classes_,\n","  yticklabels=model_td_only.classes_,\n",")\n","\n","# Label the X and Y axes\n","plt.ylabel('Actual Label of News Genre')\n","plt.xlabel('Predicted Label of News Genre')\n","\n","# Let's plug in our overall accuracy measures into the the title\n","all_sample_title = f\"Short Description Count Model\\nOverall Accuracy Score: {accuracy_td_only}\\nMean Reciprocal Rank: {mrr_at_k_td_only}\"\n","plt.title(\n","  all_sample_title,\n","  size=15\n",")\n","\n","# Ok, output time!\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.4 Explore and Assess Categories/Classes with True Positive Rates, False Positive Rates, etc."]},{"cell_type":"markdown","metadata":{},"source":["Based on the above overall accuracy score and heatmap results per category, you can begin to identify news genre categories (or classes) that may be performing well versus categories that are not performing well. You can use the following code below to explore those performance biases per news genre in the corpus/model.\n","\n","As you work through the code below, take notes as you compare the overall accuracy score of the model with specific optimal threshold scores, and be sure to compare those results against the EDA work that you conducted earlier in the notebook. "]},{"cell_type":"markdown","metadata":{},"source":["Our LR model has 31 classes (news genres), so it's a \"multi-class\" LR model. To help us explore the predictive accuracy across each class, we must first use scikit-learn's `LabelBinarizer()` to enable us to compute the True Positive Rates (TPR) and False Positive Rates (FPR) for each class/category.\n","\n","The 3 variables assigned below will help us process the training and testing data to do so."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["label_binarizer = LabelBinarizer().fit(Y_train)\n","y_onehot_test = label_binarizer.transform(Y_test)\n","y_onehot_test.shape  # (n_samples, n_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_onehot_test"]},{"cell_type":"markdown","metadata":{},"source":["##### ROC Curve & AUC"]},{"cell_type":"markdown","metadata":{},"source":["An *ROC curve* (Receiver Operating Characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots the False Positive (x) and True Positive (y) rates at their different classification thresholds to help us visualize the performance of the model at different potential thresholds. In effect, this visual can help you isolate the performance of some classes over others in relationship with the original EDA work that you have conducted.\n","\n","You can also use ROC curve plots to see how when you lower the classification threshold, the model will classify more items as positive and increase both False Positives and True Positives.\n","\n","Also, AUC stands for Area Under the Curve. It aggregates all of the probable classification thresholds as a value between 0 and 1. \n","- 0 == The model is wrong 100% of the time\n","- 1 == The model is correct 100% of the time\n","\n","Think of it like the spectrum between 0 and 1 below, where the random positive predictions are positioned to the right of the random negative predictions. So, if the AUC score is `0.714`, then the probability that `p` is positioned to the right of `n` is `71.4%`.\n","\n","```\n","Ouput of the LR Model\n","n = Actual Negative Prediction\n","p = Actual Positive Prediction\n","\n"," nnnnnnnnnnnnnnnnnnnnnpnnnnpnnpnppnnpnppppppppppppp\n","|--------------------------------------------------|\n","0                                                  1\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def roc_curve_per_category(list_classes):\n","  '''\n","  ## Purpose\n","  `roc_curve_per_category`: Combine all of the ROC () values for each category in one DataFrame\n","\n","  ## Parameters\n","  - `list_classes` (List): A list of the classes in the trained model\n","\n","  ## Return Value(s)\n","  - `df_fpr_tpr` (pandas DataFrame): Dataframe with the following values per row:\n","    - `'Class'`: Class from the model for instance\n","    - `'FPR'`: False Positive Rate for instance\n","    - `'TPR'`: True Positive Rate for instance\n","    - `'Threshold_Value'`: Threshold Value for instance\n","    - `'Threshold_Optimal'`: Optimal threshold value for the class as a whole\n","    - `'GMean_Optimal'`: Optimal GMean value for the class as a whole\n","    - `'FPR_Optimal'`: Optimal FPR value for the class as a whole\n","    - `'TPR_Optimal'`: Optimal TPR value for the class as a whole\n","  '''\n","  list_dicts_classes_roc = []\n","\n","  for class_cat in list_classes:\n","    # get class category (news genre)\n","    class_id = np.flatnonzero(label_binarizer.classes_ == class_cat)[0]\n","    Y_probability_a_td_only[:, class_id]\n","\n","    fpr, tpr, thresholds = roc_curve(y_onehot_test[:, class_id], Y_probability_a_td_only[:, class_id])\n","    \n","    # Calculate the Geometric-Mean\n","    geometric_mean = np.sqrt(tpr * (1 - fpr))\n","    \n","    # Find the optimal threshold\n","    index = np.argmax(geometric_mean)\n","    threshold_optimal = round(thresholds[index], ndigits=4)\n","    gmean_optimal = round(geometric_mean[index], ndigits=4)\n","    fpr_optimal = round(fpr[index], ndigits=4)\n","    tpr_optimal = round(tpr[index], ndigits=4)\n","\n","    for i in range(0, len(fpr)):\n","      list_dicts_classes_roc.append({\n","        'Class': class_cat,\n","        'FPR': fpr[i],\n","        'TPR': tpr[i],\n","        'Threshold_Value': thresholds[i],\n","        'Threshold_Optimal': threshold_optimal,\n","        'GMean_Optimal': gmean_optimal,\n","        'FPR_Optimal': fpr_optimal,\n","        'TPR_Optimal': tpr_optimal,\n","      })\n","\n","  df_fpr_tpr = pd.DataFrame(list_dicts_classes_roc)\n","\n","  return df_fpr_tpr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_all_classes_roc_values = roc_curve_per_category(model_td_only.classes_)\n","df_all_classes_roc_values.info()"]},{"cell_type":"markdown","metadata":{},"source":["Let's isolate the optimal values per class/category/news genre."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_all_classes_optimal_roc_values = df_all_classes_roc_values.drop_duplicates(subset=['Class']).reset_index()[['Class','Threshold_Optimal','GMean_Optimal','FPR_Optimal','TPR_Optimal']]\n","\n","df_all_classes_optimal_roc_values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["type(Y_probability_a_td_only)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_class_roc_curve(class_of_interest, Y_one_vs_all, Y_prob_a, df_class_row):\n","    '''\n","    ## Purpose\n","    `plot_class_roc_curve`: Automate the visualizing of the FPR vs TPR of a particular class/category using the `RocCurveDisplay` function\n","\n","    ## Parameters\n","    - `class_of_interest` (String): Category/class name to isolate\n","    - `Y_one_vs_all` (numpy array): List (array) of binarized values produced by `LabelBinarizer`'s `.transform()` on the Y_test data set.\n","        - NOTE: Function assumes that `label_binarizer` as an object with that name has been initialized and assigned as such.\n","    - `Y_prob_a` (numpy array): Array of probability estimates for each class/category.\n","    - `df_class_row` (row from pandas Dataframe): This dataframe row includes the optimal\n","\n","    ## Return Values\n","    - None. Instead, it \"shows\" the matplotlib plot object.\n","    '''\n","    class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n","\n","    RocCurveDisplay.from_predictions(\n","        Y_one_vs_all[:, class_id],\n","        Y_prob_a[:, class_id],\n","        name=f\"{class_of_interest} vs the rest\",\n","        color=\"white\",\n","        plot_chance_level=True,\n","    )\n","\n","    # Plot best threshold\n","    # x = FPR_Optimal, y = TPR_Optimal\n","    plt.plot(\n","        df_class_row.FPR_Optimal, \n","        df_class_row.TPR_Optimal,\n","        marker=\"o\",\n","        markerfacecolor='white',\n","        markeredgecolor='black',\n","        markersize=10,\n","    )\n","    opt_x = df_class_row.FPR_Optimal\n","    opt_y = df_class_row.TPR_Optimal\n","    plt.annotate(\n","        f\"Optimal threshold for {class_of_interest} ({str(opt_y.values.tolist()[0])})\",\n","        (opt_x, opt_y), #x,y point to label\n","        xytext=(opt_x+0.03, opt_y-0.05)\n","    )\n","\n","    plt.style.use('cyberpunk')\n","    plt.xlabel(\"False Positive Rate\")\n","    plt.ylabel(\"True Positive Rate\")\n","    plt.title(f\"One-vs-Rest ROC curves:\\n{class_of_interest} vs Rest\", color='white')\n","    plt.tight_layout()\n","    mplcyberpunk.add_glow_effects()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class_of_interest = \"GREEN\"\n","plot_class_roc_curve(\n","  class_of_interest=class_of_interest,\n","  Y_one_vs_all=y_onehot_test,\n","  Y_prob_a=Y_probability_a_td_only,\n","  df_class_row=df_all_classes_optimal_roc_values.loc[df_all_classes_optimal_roc_values.Class == class_of_interest]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.5 `text_desc` only & count-based modeling assessment\n","\n","- **Accuracy**: \n","- **MRR**:\n","- **Insert Observation #1**\n","  - Enter thoughts about this observation based on the available assessment evidence above\n","- **Insert Observation #2**\n","  - Enter thoughts about this observation based on the available assessment evidence above\n","- ..."]},{"cell_type":"markdown","metadata":{"id":"MxURZ9z4lgtI"},"source":["### 4.5 Model 2 - `tfidf` features with `text_desc_headline` - short description + headline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77839,"status":"ok","timestamp":1649709433418,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"0UCVmsJplgtI","outputId":"5bb3d97a-f5c1-4998-827e-096ec6155215"},"outputs":[],"source":["field='text_desc_headline'\n","feature_rep='tfidf'\n","top_k=3\n","\n","model_tfidf_tdh,transformer_tfidf_tdh,accuracy_tfidf_tdh,mrr_at_k_tfidf_tdh,X_tfidf_tdh_train,X_tfidf_tdh_test,Y_tfidf_tdh_test,Y_tfidf_tdh_train,preds_tfidf_tdh,eval_items_tfidf_tdh = train_model(\n","  df,\n","  field=field,\n","  feature_rep=feature_rep,\n","  top_k=top_k\n",")\n","\n","print(f\"\\n\\nSimple Mean Average Model Accuracy = {accuracy_tfidf_tdh}\\nMean Reciprocal Rank = {mrr_at_k_tfidf_tdh}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.5.5 `text_desc`, `headline`, & TF-IDF-based modeling assessment\n","\n","Now, use the same assessment techniques as in the `text_desc` only model to assess this model that uses TF-IDF + a combination of the short description and headline values as features for the model.\n","\n","Write your observations and evidence below.\n","\n","- **Accuracy**: \n","- **MRR**:\n","- **Insert Observation #1**\n","  - Enter thoughts about this observation based on the available assessment evidence above\n","- **Insert Observation #2**\n","  - Enter thoughts about this observation based on the available assessment evidence above\n","- ..."]},{"cell_type":"markdown","metadata":{"id":"yq8ySg7jlgtJ"},"source":["### 4.6 Model 3 - `tfidf` features with `text_desc_headline_url`: description, headline, *and* url"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":121811,"status":"ok","timestamp":1649709555223,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"HIshxNadlgtJ","outputId":"38df0398-521e-45ea-f9c8-e2ba27459290"},"outputs":[],"source":["field='text_desc_headline_url'\n","feature_rep='tfidf'\n","top_k=3\n","\n","model_tfidf_all,transformer_tfidf_all,accuracy_tfidf_all,mrr_at_k_tfidf_all,X_tfidf_all_train,X_tfidf_all_test,Y_tfidf_all_test,Y_tfidf_all_train,preds_tfidf_all,eval_items_tfidf_all = train_model(\n","  df,\n","  field=field,\n","  feature_rep=feature_rep,\n","  top_k=top_k\n",")\n","\n","print(\"\\nAccuracy={0}; MRR={1}\".format(accuracy_tfidf_all,mrr_at_k_tfidf_all))"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.6.5 `text_desc`, `headline`, `URL`, & TF-IDF-based modeling assessment\n","\n","Now, use the same assessment techniques as the previous models to assess this third model that uses TF-IDF + a combination of the short description, headline, and URL values as features for the model.\n","\n","Write your observations and evidence below.\n","\n","- **Accuracy**: \n","- **MRR**:\n","- **Insert Observation #1**\n","  - Enter thoughts about this observation based on the available assessment evidence above\n","- **Insert Observation #2**\n","  - Enter thoughts about this observation based on the available assessment evidence above\n","- ..."]},{"cell_type":"markdown","metadata":{"id":"62uJ-ivklgtJ"},"source":["## 5. Check Predictions on Unseen Articles"]},{"cell_type":"markdown","metadata":{},"source":["Try from articles that aren't from HuffPost: CNN, Fox, MSNBC, etc.\n","\n","Remember that the algorithm will return the top `k` probabilities. I've created a default of 3 for `k`. The first value in the list is the top returned predicted value."]},{"cell_type":"markdown","metadata":{},"source":["### 5.1 Model 1 trained by counts method &amp; the features from the short description only"]},{"cell_type":"markdown","metadata":{},"source":["This uses Model 1: \n","\n","- Model: `model_td_only`\n","- Transformer: `transformer_td_only`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1649709555225,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"kBRHmMALlgtJ","outputId":"63d93f5c-0214-429c-cb27-e1d98d7f987b"},"outputs":[],"source":["# https://www.cnn.com/2019/07/19/health/astronaut-exercise-iv-faint-scn/index.html\n","test_features = transformer_td_only.transform([\"Exercise in space keeps astronauts from fainting when they return to Earth, study says.\"])\n","get_top_k_predictions(\n","  model_td_only, \n","  test_features,\n","  3\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### 5.2 Model 2 trained by TF-IDF &amp; the features from the short description &amp; headline"]},{"cell_type":"markdown","metadata":{},"source":["This uses Model 2:\n","\n","- Model: `model_tfidf_tdh`\n","- Transformer: `transformer_tfidf_tdh`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1649709555224,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"ivvAe28plgtJ","outputId":"4c455efa-fbcd-4d6c-d7d7-18b25cc09592"},"outputs":[],"source":["# URL: https://www.network.com/enter/url/to/story/here.html\n","\n","test_features=transformer_tfidf_tdh.transform(\n","    [\n","     \"Enter headling here from story above\"\n","    ]\n",")\n","\n","get_top_k_predictions(\n","  model_tfidf_tdh,\n","  test_features,\n","  3\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### 5.3 Model 3 trained by TF-IDF &amp; the features from the short description, headline &amp; URL"]},{"cell_type":"markdown","metadata":{},"source":["This uses Model 3:\n","\n","- Model: `model_tfidf_all`\n","- Transformer: `transformer_tfidf_all`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# URL: https://www.network.com/enter/url/to/story/here.html\n","test_features=transformer_tfidf_tdh.transform(\n","    [\n","     \"Enter headling here from story above\"\n","    ]\n",")\n","\n","get_top_k_predictions(\n","  model_tfidf_tdh,\n","  test_features,\n","  3\n",")"]},{"cell_type":"markdown","metadata":{"id":"In6wjZxelgtK"},"source":["## 6. Save Models for Future Use\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":287,"status":"aborted","timestamp":1649711042976,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"aF89HEoXlgtK"},"outputs":[],"source":["# Use imported `pickle` code library\n","\n","#### 1. COUNTS - 'text_desc' as features only ####\n","model_path_td_only = \"./../models/news_genre_model_binary_text_desc_only.pkl\"\n","transformer_path_td_only = \"./../models/news_genre_transformer_binary_text_desc_only.pkl\"\n","# Save both the transformer -> to encode a document and the model itself to make predictions based on the weight vectors \n","pickle.dump( model_td_only, open(model_path_td_only, 'wb') )\n","pickle.dump( transformer_td_only, open(transformer_path_td_only,'wb') )\n","\n","#### 2. TF-IDF - 'text_desc_headline' as features ####\n","model_path_tdh = \"./../models/news_genre_model_tfidf_tdh.pkl\"\n","transformer_path_tdh = \"./../models/news_genre_transformer_tfidf_tdh.pkl\"\n","# Save both the transformer -> to encode a document and the model itself to make predictions based on the weight vectors \n","pickle.dump( model_tfidf_tdh, open(model_path_tdh, 'wb') )\n","pickle.dump( transformer_tfidf_tdh, open(transformer_path_tdh,'wb') )\n","\n","#### 3. TF-IDF - 'text_desc_headline_url' as features ####\n","model_path_all = \"./../models/news_genre_model_tfidf_all.pkl\"\n","transformer_path_all = \"./../models/news_genre_transformer_tfidf_all.pkl\"\n","# Save both the transformer -> to encode a document and the model itself to make predictions based on the weight vectors \n","pickle.dump( model_tfidf_all, open(model_path_all, 'wb') )\n","pickle.dump( transformer_tfidf_all, open(transformer_path_all,'wb') )\n"]},{"cell_type":"markdown","metadata":{"id":"J9HN07JXlgtK"},"source":["## 7. Use Loaded Model"]},{"cell_type":"markdown","metadata":{},"source":["With pickle, we can now also load the model and use it without needing to retrain everything. A good time saver to consider for your team's final project!"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":288,"status":"aborted","timestamp":1649711042977,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"VXBz2xQ2lgtK"},"outputs":[],"source":["# Load Model 2 for reference\n","# Note my variable assignments include \"loaded_\" so I can distinguish variable assignments, if necessary.\n","\n","model_path_tdh = \"./../models/news_genre_model_tfidf_tdh.pkl\"\n","transformer_path_tdh = \"./../models/news_genre_transformer_tfidf_tdh.pkl\"\n","\n","loaded_model_tfidf_tdh = pickle.load(open(model_path_tdh, 'rb'))\n","loaded_transformer_tfidf_tdh = pickle.load(open(transformer_path_tdh, 'rb'))\n","\n","# URL: https://www.network.com/enter/url/to/story/here.html\n","loaded_test_features_tfidf_tdh = loaded_transformer_tfidf_tdh.transform([\"Enter headling here from story above\"])\n","get_top_k_predictions(loaded_model_tfidf_tdh, loaded_test_features_tfidf_tdh, 3)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusion"]},{"cell_type":"markdown","metadata":{},"source":["Overall, do your best to have understood this LR modeling/training process: the goals for the model in relationship to the original data set used. By documenting your observations and using Python to conduct EDA + model assessments, you can complete the following work that you will need to repeat for your final project:\n","\n","1. Identify potential boundaries, biases, and limits of the data in relationship to your developing and changing goals for modeling\n","2. Conversely, identify potential possibilities and affordances of the data in relationship to your developing and changing goals for modeling\n","3. Documenting these biases, affrodances, and changes in goals\n","4. Use this notebook material to develop a Model Card that communicates the aforementioned material in a more concise and helpful manner"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Text Classification with Logistic Regression.ipynb","provenance":[]},"interpreter":{"hash":"ca9752e44562eac9eb8af20ed27ecdaee8307d29d279d171455889baf1c568d6"},"kernelspec":{"display_name":"3.8.3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.8"}},"nbformat":4,"nbformat_minor":0}
